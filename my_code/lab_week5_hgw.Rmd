---
title: "lab5_hgw"
author: "Hannah Garcia"
date: "2/7/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# install packages
library(tidyverse)
library(tsibble)
library(feasts)
library(fable)
library(here)
```

```{r}
# read in the data
energy <- read_csv(here("data", "energy.csv"))
```

```{r}
# wrangle data
energy_ts <- energy %>% 
  mutate(date = tsibble::yearmonth(month)) %>% 
  as_tsibble(key = NULL, index = date)
```

# Exploratory time series visualization 
```{r}
# raw data
ggplot(data = energy_ts, aes(x = date, y = res_total)) +
  geom_line() +
  labs(y = "Residential energy consumption \n (Trillion BTU)")
```

```{r}
# seasonplot
energy_ts %>% 
  gg_season(y = res_total) +
  theme_minimal() +
  labs(x = "month",
       y = "residential energy consumption (trillion BTU)")
```
- highest residential energy usage is around December / January / February
- there is a secondary peak around July & August (that's the repeated secondary peak we see in the original time series graph)
- We can also see that the prevalence of that second peak has been increasing over the course of the time series: in 1973 (orange) there was hardly any summer peak. In more recent years (blue/magenta) that peak is much more prominent.

## subseries plots
```{r}
energy_ts %>% gg_subseries(res_total)
```
- Our takeaway here is similar: there is clear seasonality (higher values in winter months), with an increasingly evident second peak in June/July/August. This reinforces our takeaways from the raw data and seasonplots.

## Decomposition (here by STL)
```{r}
# Find STL decomposition
dcmp <- energy_ts %>% 
  model(STL(res_total ~ season()))

# view the components
components(dcmp)

# visualize the decomposed components
components(dcmp) %>% autoplot() +
  theme_minimal()
```

## Autocorrelation function (ACF)
```{r}
energy_ts %>% 
  ACF(res_total) %>% 
  autoplot()
```
- And yep, we see that observations separated by 12 months are the most highly correlated, reflecting strong seasonality we see in all of our other exploratory visualizations.

# Forecasting by Holt-Winters exponential smoothing
```{r}
# create the model
energy_fit <- energy_ts %>% 
  model(
    ets = ETS(res_total ~ season("M"))
  )

# Forecast using the model 10 years into the future:
energy_forecast <- energy_fit %>% 
  forecast(h = "10 years")

# Plot just the forecasted values (with 80 & 95% CIs):
energy_forecast %>% 
  autoplot()
```

```{r}
# or plot it added to the original data
energy_forecast %>% 
  autoplot(energy_ts)
```

## Assessing residuals
- We can use broom::augment() to append our original tsibble with what the model predicts the energy usage would be based on the model. Let’s do a little exploring through visualization.

```{r}
# Append the predicted values (and residuals) to original energy data
energy_predicted <- broom::augment(energy_fit)

# use view(energy_predicted) to see the resulting data frame
```

- Now, plot the actual energy values (res_total), and the predicted values (stored as .fitted) atop the

```{r}
ggplot(data = energy_predicted) +
  geom_line(aes(x = date, y = res_total)) +
  geom_line(aes(x = date, y = .fitted), color = "red")
```

- Now let’s explore the residuals. Remember, some important considerations: Residuals should be uncorrelated, centered at 0, and ideally normally distributed. One way we can check the distribution is with a histogram:

```{r}
ggplot(data = energy_predicted, aes(x = .resid)) +
  geom_histogram()
```

# Spatial data wrangling, visualization, and a variogram
## California county outlines (polygons)

```{r}
library(sf)
ca_counties <- read_sf(here("data", "ca_counties", "CA_Counties_TIGER2016.shp"))
```

```{r}
ca_subset <- ca_counties %>% 
  select(NAME, ALAND) %>% 
  rename(county_name = NAME, land_area = ALAND)
```

## Check and set the CRS
```{r}
ca_subset %>%  st_crs()
```

```{r}
ggplot(data = ca_subset) +
  geom_sf(aes(fill = land_area), color = "white", size = 0.1) +
  theme_void() +
  scale_fill_gradientn(colors = c("cyan","blue", "purple"))
```

## Invasive red sesbania records

```{r}
sesbania <- read_sf(here("data","red_sesbania","ds80.shp"))

# check the CRS
sesbania %>% st_crs()
```

- Notice that this CRS is different from the California counties CRS, so we’ll want to update it to match. Use st_transform() to update the CRS:

```{r}
sesbania <- st_transform(sesbania, 3857)

# then check it:
sesbania %>%  st_crs()
```

## plot them together

```{r}
ggplot() +
  geom_sf(data = ca_subset) +
  geom_sf(data = sesbania, size = 1, color = "red")
```

- Let’s say we want to find the count of red sesbania observed locations in this dataset by county. How can I go about joining these data so that I can find counts? Don’t worry…st_join() has you covered for spatial joins!

```{r}
# observations by county
ca_sesbania <- ca_subset %>% 
  st_join(sesbania)

# find records of observations
sesbania_counts <- ca_sesbania %>% 
  count(county_name)

# chloropleth graph 
ggplot(data = sesbania_counts) +
  geom_sf(aes(fill = n), color = "white", size = 0.1) +
  scale_fill_gradientn(colors = c("lightgray","orange","red")) +
  theme_minimal() +
  labs(fill = "Number of S. punicea records")
```

```{r}
# Subset of sesbania point locations only in Solano County
solano_sesbania <- sesbania %>% 
  filter(COUNTY == "Solano")

# Only keep Solano polygon from California County data
solano <- ca_subset %>% 
  filter(county_name == "Solano")

ggplot() +
  geom_sf(data = solano) +
  geom_sf(data = solano_sesbania)
```

## Making an interactive map with {tmap}
```{r}
library(tmap)

# Set the viewing mode to "interactive":
tmap_mode(mode = "view")

# Then make a map (with the polygon fill color updated by variable 'land_area', updating the color palette to "BuGn"), then add another shape layer for the sesbania records (added as dots):
tm_shape(ca_subset) +
  tm_fill("land_area", palette = "BuGn") +
  tm_shape(sesbania) +
  tm_dots()
```




